{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86e5efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8ff0a",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91658039",
   "metadata": {},
   "source": [
    "Below is an implemented KMeans algorithm on Word2Vec embeddings of the WikiHow article data. KMeans is an unsupervised learning algorithm, meaning we do not care about the \"Summary\" columns during training. The pipeline is summarized as follows:\n",
    "- Preprocess Text (Remove stopwords, short words, punctuation, etc.)\n",
    "- Tokenize Data\n",
    "- Embed data into Word2Vec\n",
    "- Pass embedded vectors into KMeans\n",
    "\n",
    "We perform this process for each article and average the rouge score to evaluate our metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66db768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Here we set text to lower case, remove plurals, \n",
    "    expand contractions, remove punctuation, remove stopwords, and remove short words \n",
    "    (could also remove parentheticals)\"\"\"\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    ret = text.lower()\n",
    "    ret = contractions.fix(text)\n",
    "    ret = re.sub(r'\\([^)]*\\)', '', ret)\n",
    "    ret = re.sub('\"','', ret)\n",
    "    ret = re.sub(r\"'s\\b\",\"\", ret)\n",
    "    ret = re.sub(\"[^a-zA-Z]\", \" \", ret) \n",
    "    \n",
    "    #Remove any words shorter than 2 letters\n",
    "    tokens = [w for w in ret.split() if not w in stop]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                 \n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29acb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n",
    "data = pd.read_csv('data/cleaned_data.csv')\n",
    "data = data[['Summary', 'Text']]\n",
    "\n",
    "#Create corpus for TF-IDF of summaries\n",
    "#corpus = data[\"Text\"]\n",
    "#corpus = corpus.apply(clean_text)\n",
    "\n",
    "#Create Sample Text Doc\n",
    "test = data['Text'][0]\n",
    "text = test.replace('\\\\', '').replace('/', '').replace('.,', '.').replace('.;,', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d05f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data['Text'][0]\n",
    "label = data['Summary'][0]\n",
    "text = test.replace('\\\\', '').replace('/', '').replace('.,', '.').replace('.;,', '.')\n",
    "lbl = label.replace('\\\\', '').replace('/', '').replace('.,', '. ').replace('.;,', '. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d98e11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sent_tokenize(text)\n",
    "clean = []\n",
    "for sen in sentence:\n",
    "    clean.append(clean_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7255150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "class KMeans_Summ():\n",
    "    def __init__(self, max_iters=300, n_init=10):\n",
    "        self.sentence = None\n",
    "        self.max_iters = max_iters\n",
    "        self.n_init = n_init\n",
    "    \n",
    "    def embed_article(self, min_count, vector_size, article):\n",
    "        clean=[]\n",
    "        \n",
    "        sentence = sent_tokenize(article)\n",
    "        self.sentence = sentence\n",
    "        \n",
    "        for sen in sentence:\n",
    "            clean.append(clean_text(sen))\n",
    "        all_words = [i.split() for i in clean]\n",
    "        model = Word2Vec(all_words, min_count=min_count, vector_size=vector_size)\n",
    "        \n",
    "        sent_vector=[]\n",
    "        for i in clean:\n",
    "            plus=0\n",
    "            for j in i.split():\n",
    "                plus+=model.wv[j]\n",
    "            if len(i.split()) != 0:\n",
    "                plus = plus/len(i.split())\n",
    "            sent_vector.append(plus)\n",
    "        return sent_vector\n",
    "    \n",
    "    def summarize_article(self, n_clusters, vector):\n",
    "        kmeans = KMeans(n_clusters, init='k-means++', random_state=42, max_iter = self.max_iters, n_init = self.n_init) \n",
    "        y_kmeans = kmeans.fit_predict(vector)\n",
    "\n",
    "        my_list=[]\n",
    "        #print(\"Summarizing Article...\")\n",
    "        for i in range(n_clusters):\n",
    "            my_dict={}\n",
    "\n",
    "            for j in range(len(y_kmeans)):\n",
    "                if y_kmeans[j]==i:\n",
    "                    my_dict[j] = distance.euclidean(kmeans.cluster_centers_[i],vector[j])\n",
    "            min_distance = min(my_dict.values())\n",
    "            my_list.append(min(my_dict, key=my_dict.get))\n",
    "\n",
    "        result = \"\"\n",
    "        for i in sorted(my_list):\n",
    "            result += self.sentence[i] + \" \"\n",
    "        #print(\"Finished Summarizing!\")\n",
    "        return result                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee0e3733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 200\n",
      "5 300\n",
      "5 400\n",
      "10 200\n",
      "10 300\n",
      "10 400\n",
      "15 200\n",
      "15 300\n",
      "15 400\n",
      "338040\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "#Parameter values for search\n",
    "m_iters = [200, 300, 400]\n",
    "n_init = [5, 10, 15]\n",
    "\n",
    "#Loop to store all result text and summaries\n",
    "tot_results = []\n",
    "rouge1 = []\n",
    "grid = {}\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "for init in n_init:\n",
    "    for miter in m_iters:\n",
    "        print(init, miter)\n",
    "        scores = []\n",
    "        for i, row in enumerate(data[\"Text\"][:50000]):\n",
    "            summary = data[\"Summary\"][i].replace('\\\\', '').replace('/', '').replace('.,', '.').replace('.;,', '.')\n",
    "            text = row.replace('\\\\', '').replace('/', '').replace('.,', '.').replace('.;,', '.')\n",
    "            #print(text)\n",
    "\n",
    "            if len(text) >= 1000:\n",
    "                try:\n",
    "                    summ = KMeans_Summ(miter, init)\n",
    "                    vec = summ.embed_article(1, 300, text)\n",
    "                    result = summ.summarize_article(3, vec)\n",
    "                    tot_results.append(result)\n",
    "                    #print(scorer.score(result, summary))\n",
    "                    scores.append(scorer.score(result, summary))\n",
    "                    \n",
    "                except:\n",
    "                    scores.append(0)\n",
    "        \n",
    "        for score in scores:\n",
    "            #print(\"scores\")\n",
    "            if score != 0:\n",
    "                rouge1.append(score['rouge1'].fmeasure)\n",
    "        \n",
    "        grid['{init}-{miter}'.format(init = init, miter = miter)] = np.mean(rouge1)\n",
    "\n",
    "                       \n",
    "print(len(tot_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a98cf437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'5-200': 0.24654490908978527, '5-300': 0.24654490908978527, '5-400': 0.24654490908978527, '10-200': 0.24629058450438096, '10-300': 0.2461379897531384, '10-400': 0.24603625991897665, '15-200': 0.24590532336630355, '15-300': 0.24580712095179866, '15-400': 0.24573074129607264}\n",
      "5-400\n"
     ]
    }
   ],
   "source": [
    "print(grid)\n",
    "print(max(grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e414683a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146016\n",
      "140383\n",
      "0.25337091953441254\n"
     ]
    }
   ],
   "source": [
    "rouge1 = []\n",
    "\n",
    "for score in scores:\n",
    "    if score != 0:\n",
    "        rouge1.append(score['rouge1'].fmeasure)\n",
    "\n",
    "print(len(scores))\n",
    "print(len(rouge1))\n",
    "print(np.mean(rouge1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ae928",
   "metadata": {},
   "source": [
    "# Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610af0a",
   "metadata": {},
   "source": [
    "While not essential to the KMeans above, a simple keyword extraction tool using TF-IDF is implemented below for users to tag their text as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee14b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_IDF():\n",
    "    def __init__(self, corpus):\n",
    "        self.text = corpus\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        self.cv = CountVectorizer(max_df=0.85, stop_words=self.stopwords)\n",
    "        self.wordcount = self.cv.fit_transform(corpus)\n",
    "    \n",
    "        self.transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "        self.transformer.fit(self.wordcount)\n",
    "    \n",
    "    def sort_vals(self, matrix):\n",
    "        tuples = zip(matrix.col, matrix.data)\n",
    "        return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    \n",
    "    def extract_top_k(self, feature_names, items, k=10):\n",
    "        items = items[:k]\n",
    "\n",
    "        scores = []\n",
    "        features = []\n",
    "\n",
    "        for idx, score in items:\n",
    "            scores.append(round(score, 3))\n",
    "            features.append(feature_names[idx])\n",
    "        \n",
    "        results = {}\n",
    "        for idx in range(len(features)):\n",
    "            results[features[idx]] = scores[idx]\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def extract_keywords(self, doc, k=10):\n",
    "        feature_names = self.cv.get_feature_names()\n",
    "        tf_idf_vector = self.transformer.transform(self.cv.transform([doc]))\n",
    "\n",
    "        sort_items = self.sort_vals(tf_idf_vector.tocoo())\n",
    "        keywords = self.extract_top_k(feature_names, sort_items, k)\n",
    "\n",
    "        print(\"\\nDocument\")\n",
    "        print(doc)\n",
    "        print(\"\\nKeywords\")\n",
    "        for k in keywords:\n",
    "            print(k, keywords[k])\n",
    "        \n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2a391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
